{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrupar_categorias_pequenas(df, columna, umbral=100):\n",
    "    # Contar la frecuencia de cada categoría\n",
    "    conteo_categorias = df[columna].value_counts()\n",
    "    \n",
    "    # Identificar categorías con observaciones por debajo del umbral\n",
    "    categorias_pequenas = conteo_categorias[conteo_categorias < umbral].index\n",
    "    \n",
    "    # Mantener el mismo tipo de las categorías originales\n",
    "    tipo_original = df[columna].dtype.type\n",
    "    \n",
    "    # Reemplazar esas categorías con 'Otros', asegurando el mismo tipo de dato\n",
    "    df[columna] = df[columna].apply(lambda x: tipo_original('Otros') if x in categorias_pequenas else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    if min(r, k) == 1:\n",
    "        return np.nan  # Evita dividir por cero\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "def expand_list(df, column):\n",
    "    # Crear un conjunto de categorías\n",
    "    categories = set()\n",
    "    \n",
    "    # Recorrer todas las listas y agregar cada categoría única al conjunto\n",
    "    for row in df[column]:\n",
    "        if pd.notna(row):\n",
    "            # Verificar si el valor es una lista; si es string, se convierte en lista\n",
    "            if isinstance(row, str):\n",
    "                try:\n",
    "                    row_list = eval(row)  # Transforma la string en lista\n",
    "                except:\n",
    "                    row_list = [row]  # Si no es lista, lo trata como valor único\n",
    "            else:\n",
    "                row_list = row if isinstance(row, list) else [row]\n",
    "            \n",
    "            # Añadir los elementos de la lista a las categorías\n",
    "            categories.update(map(str, row_list))  # Convertir todos los elementos a string\n",
    "\n",
    "    # Crear un DataFrame temporal para almacenar las nuevas columnas\n",
    "    new_columns = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Llenar el DataFrame temporal con las nuevas columnas\n",
    "    for category in categories:\n",
    "        # Crear el nombre de la nueva columna con el sufijo del nombre de la columna original\n",
    "        new_column_name = f\"{category}_{column}\"\n",
    "        new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
    "\n",
    "    # Concatenar el nuevo DataFrame con las columnas originales\n",
    "    df = pd.concat([df.drop(columns=[column]), new_columns], axis=1)\n",
    "\n",
    "    return categories, df\n",
    "\n",
    "\n",
    "def categorizar_hora(hora):\n",
    "        hora = int(hora.split(':')[0])\n",
    "        if 0 <= hora < 12:\n",
    "            return 'mañana'\n",
    "        elif 12 <= hora < 18:\n",
    "            return 'tarde'\n",
    "        else:\n",
    "            return 'noche'\n",
    "\n",
    "def convertir_auction_time(df):\n",
    "    \n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s').dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    # Crear una nueva columna con las categorías de tiempo\n",
    "    df['parte_del_dia'] = df['auction_time'].apply(categorizar_hora)\n",
    "    \n",
    "    # Crear columnas binarias para cada parte del día\n",
    "    df['mañana'] = (df['parte_del_dia'] == 'mañana').astype(int)\n",
    "    df['tarde'] = (df['parte_del_dia'] == 'tarde').astype(int)\n",
    "    df['noche'] = (df['parte_del_dia'] == 'noche').astype(int)\n",
    "    \n",
    "    # Opcional: Eliminar la columna 'parte_del_dia' si no se necesita\n",
    "    df.drop(['parte_del_dia', 'auction_time'], axis=1, inplace=True)\n",
    "\n",
    "    return ['mañana', 'tarde', 'noche'], df\n",
    "\n",
    "def imputar_nan_como_otros(df):\n",
    "    for columna in df.columns:\n",
    "        if df[columna].isnull().any():  # Verificar si hay NaN en la columna\n",
    "            tipo_original = df[columna].dtype  # Guardar el tipo original de la columna\n",
    "            # Imputar 'Otros' y asegurarse de que el tipo de dato se mantiene\n",
    "            df[columna] = df[columna].fillna(tipo_original.type('Otros'))\n",
    "    return df\n",
    "\n",
    "def data_cleaning(df):\n",
    "    all_categories = set()\n",
    "\n",
    "    categorical_features = [col for col in df.select_dtypes(include=['object']).columns.tolist() if col not in ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']]\n",
    "\n",
    "    categories, df = convertir_auction_time(df)\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    categories, df = expand_list(df, 'auction_list_0')\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    categories, df = expand_list(df, 'action_list_1')\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    categories, df = expand_list(df, 'action_list_2')\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    for col in categorical_features:\n",
    "        df = agrupar_categorias_pequenas(df, col, umbral=100) \n",
    "\n",
    "    return all_categories, df\n",
    "\n",
    "def ajustar_columnas_test(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Asegura que las columnas del conjunto de test coincidan con las del conjunto de entrenamiento,\n",
    "    con la excepción de que 'Label' no se puede borrar de train y 'id' no se puede borrar de test.\n",
    "    \n",
    "    Si faltan columnas en el conjunto de test, se agregan con valor 0.\n",
    "    Si sobran columnas en el conjunto de test que no están en el de entrenamiento, se eliminan.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df: DataFrame del conjunto de entrenamiento (con las columnas transformadas)\n",
    "    - test_df: DataFrame del conjunto de test (sin transformar)\n",
    "    \n",
    "    Returns:\n",
    "    - test_df: DataFrame del conjunto de test con las columnas ajustadas.\n",
    "    \"\"\"\n",
    "    # Encontrar las columnas que están en train pero no en test, excepto 'Label'\n",
    "    missing_cols = set(train_df.columns) - set(test_df.columns) - {'Label'}\n",
    "\n",
    "    # Encontrar las columnas que están en test pero no en train, excepto 'id'\n",
    "    extra_cols = set(test_df.columns) - set(train_df.columns) - {'id'}\n",
    "\n",
    "    # Agregar las columnas faltantes en el conjunto de test con valor 0\n",
    "    for col in missing_cols:\n",
    "        test_df[col] = 0\n",
    "\n",
    "    # Eliminar las columnas extra del conjunto de test, excepto 'id'\n",
    "    test_df = test_df.drop(columns=extra_cols)\n",
    "\n",
    "    # Asegurarse de que las columnas estén en el mismo orden que en el conjunto de entrenamiento\n",
    "    test_df = test_df[train_df.columns.difference(['Label']).tolist() + ['id']]\n",
    "\n",
    "    return test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_21 = pd.read_csv(\"data/ctr_21.csv\")\n",
    "# train_data_20 = pd.read_csv(\"data/ctr_20.csv\")\n",
    "# train_data_19 = pd.read_csv(\"data/ctr_19.csv\")\n",
    "# train_data_18 = pd.read_csv(\"data/ctr_18.csv\")\n",
    "# train_data_17 = pd.read_csv(\"data/ctr_17.csv\")\n",
    "# train_data_16 = pd.read_csv(\"data/ctr_16.csv\")\n",
    "# train_data_15 = pd.read_csv(\"data/ctr_15.csv\")\n",
    "# train_data_combined = pd.concat([train_data_21, train_data_20, train_data_19, train_data_18, train_data_17, train_data_16, train_data_15], axis=0)\n",
    "# train_data_combined = pd.concat([train_data_21, train_data_20], axis=0)\n",
    "train_data_combined = pd.read_csv(\"data/ctr_21.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43992294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducir el conjunto de datos mientras se mantiene la distribución de 'label'\n",
    "train_data_combined, _ = train_test_split(train_data_combined, train_size=100000, stratify=train_data_combined['Label'], random_state=42)\n",
    "\n",
    "# Verificar la distribución en el conjunto reducido\n",
    "print(train_data_combined['Label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caracteristicas principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la cantidad de filas del dataset combinado\n",
    "print(f\"Cantidad de filas en el dataset combinado: {train_data_combined.shape[0]}\")\n",
    "\n",
    "# Ver porcentaje de clics vs no clics en la columna Label\n",
    "label_counts = train_data_combined['Label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPorcentaje de clics (1) y no clics (0):\")\n",
    "print(label_counts)\n",
    "\n",
    "# Cantidad de clics (1) y no clics (0)\n",
    "label_counts_abs = train_data_combined['Label'].value_counts()\n",
    "print(\"\\nCantidad de clics (1) y no clics (0):\")\n",
    "print(label_counts_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico de las características numéricas\n",
    "print(train_data_combined.describe())\n",
    "\n",
    "# Resumen de las características categóricas\n",
    "print(train_data_combined.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el porcentaje de valores faltantes por columna\n",
    "missing_data = train_data_combined.isnull().mean() * 100\n",
    "print(missing_data[missing_data > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en numéricos y categóricos\n",
    "numeric_features = train_data_combined.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "categorical_features = train_data_combined.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columna por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos\n",
    "for column in train_data_combined.columns:\n",
    "    unique_values = train_data_combined[column].unique()\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos y la cantidad de veces que aparecen\n",
    "for column in train_data_combined.columns:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(train_data_combined[column].value_counts())\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar un conjunto vacío para almacenar los entity ids únicos\n",
    "unique_entity_ids_2 = set()\n",
    "\n",
    "# Recorrer cada fila de la columna 'action_list_2'\n",
    "for lista in train_data_combined['action_list_2'].dropna():\n",
    "    # Convertir el string de la lista a una lista real y agregar cada valor al conjunto\n",
    "    unique_entity_ids_2.update(eval(lista))  # eval transforma la string en lista si es necesario\n",
    "\n",
    "# Convertir el conjunto en una lista ordenada\n",
    "unique_entity_ids_2 = sorted(unique_entity_ids_2)\n",
    "\n",
    "# Mostrar los entity ids únicos\n",
    "print(unique_entity_ids_2)\n",
    "print(len(unique_entity_ids_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar un conjunto vacío para almacenar los entity ids únicos\n",
    "unique_entity_ids = set()\n",
    "\n",
    "# Recorrer cada fila de la columna 'action_list_1'\n",
    "for lista in train_data_combined['action_list_1'].dropna():\n",
    "    # Convertir el string de la lista a una lista real y agregar cada valor al conjunto\n",
    "    unique_entity_ids.update(eval(lista))  # eval transforma la string en lista si es necesario\n",
    "\n",
    "# Convertir el conjunto en una lista ordenada\n",
    "unique_entity_ids = sorted(unique_entity_ids)\n",
    "\n",
    "# Mostrar los entity ids únicos\n",
    "print(unique_entity_ids)\n",
    "print(len(unique_entity_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la cantidad de valores NaN en cada columna\n",
    "nan_counts = train_data_combined.isna().sum()\n",
    "\n",
    "# Filtrar columnas que tienen al menos un NaN\n",
    "nan_counts_filtered = nan_counts[nan_counts > 0]\n",
    "\n",
    "# Mostrar las columnas con valores NaN de mayor a menor\n",
    "print(nan_counts_filtered.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos\n",
    "for column in train_data_combined.columns:\n",
    "    unique_values = train_data_combined[column].unique()\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories, train_data_combined = data_cleaning(train_data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el dataset modificado como CSV antes de aplicar One-Hot Encoding\n",
    "train_data_combined.to_csv('train_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos\n",
    "for column in train_data_combined.columns:\n",
    "    unique_values = train_data_combined[column].unique()  # Obtener los valores únicos para cada columna\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined.isnull().sum()[train_data_combined.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                              # 20% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Crear el Target Encoder para las columnas categóricas\n",
    "target_encoder = ce.TargetEncoder(cols=categorical_features)\n",
    "\n",
    "# Ajustar y transformar los datos de entrenamiento\n",
    "X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transformar los datos de validación\n",
    "X_val_encoded = target_encoder.transform(X_val)\n",
    "\n",
    "# Crear un imputador que llene los valores NaN con la media de cada columna\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Aplicar el imputador a los datos de entrenamiento\n",
    "X_train_encoded = imputer.fit_transform(X_train_encoded)\n",
    "\n",
    "# También aplica el imputador al conjunto de validación\n",
    "X_val_encoded = imputer.transform(X_val_encoded)\n",
    "\n",
    "# Convertir los resultados a DataFrame y asignar las columnas originales\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=X_train.columns)\n",
    "X_val_encoded = pd.DataFrame(X_val_encoded, columns=X_val.columns)\n",
    "\n",
    "# Ver los primeros datos\n",
    "print(X_train_encoded.head())\n",
    "print(X_val_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de testeo\n",
    "test_data = pd.read_csv(\"data/ctr_test.csv\")\n",
    "\n",
    "# Eliminar las columnas no deseadas\n",
    "test_data = test_data.drop(columns=columns_to_drop)\n",
    "\n",
    "test_data_encoded = target_encoder.transform(test_data.drop(columns=[\"id\"]))\n",
    "test_data_encoded = imputer.transform(test_data_encoded)  # Imputar los valores faltantes en el set de testeo\n",
    "\n",
    "# Convertir los resultados a DataFrame y asignar las columnas originales\n",
    "test_data_encoded = pd.DataFrame(test_data_encoded, columns=test_data.drop(columns=[\"id\"]).columns)\n",
    "\n",
    "# comparar tamaño de columnas\n",
    "print(X_train_encoded.columns)\n",
    "print(test_data_encoded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined = pd.read_csv('train_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'-2560', '-2606', '-5469', '-5470', '-5471', '-5559', '-5560', '-5576', '-5577', '-5578', '-5579', '-5603', '-5604', '-5605', '-5613', '-5736', '-5902', '-6118', '-6119', '-6125', '-6217', '-6218', '-6219', '-6220', '-6223', '-6224', '-6226', '-6309', '-6454', '-6543', '-6544', '-6547', '-6548', '-6613', '-6614', '-6615', '-6617', '-6618', '-6620', '-6621', '-6770', '-6774', '-6775', '-6779', '-6780', '-6800', '-6823', '-6824', '-6848', '-6849', '-6850', '-6865', '-6866', '-6867', '-6871', '-6874', '-6875', '-6876', '-6902', '-6903', '-6904', '-6905', '-6929', '-6938', '-6946', '-7033', '-7111', '-7112', '-7126', '-7127', '-7143', '-7190', '-7194', '-7195', '-7196', '-7199', '-7263', '-7264', '-7265', '2560', '2606', '5469', '5470', '5471', '5559', '5560', '5576', '5577', '5578', '5579', '5603', '5604', '5605', '5613', '5736', '5902', '6118', '6119', '6125', '6217', '6218', '6219', '6220', '6223', '6224', '6226', '6309', '6451', '6454', '6543', '6544', '6547', '6548', '6614', '6615', '6616', '6617', '6618', '6620', '6621', '6770', '6774', '6775', '6779', '6780', '6800', '6823', '6824', '6848', '6849', '6850', '6865', '6866', '6867', '6871', '6874', '6875', '6876', '6902', '6903', '6904', '6905', '6929', '6938', '6946', '7033', '7111', '7112', '7126', '7127', '7143', '7190', '7194', '7195', '7196', '7199', '7263', '7264', '7265', 'IAB-5', 'IAB1', 'IAB1-1', 'IAB1-2', 'IAB1-3', 'IAB1-4', 'IAB1-5', 'IAB1-6', 'IAB1-7', 'IAB10', 'IAB12', 'IAB12-1', 'IAB12-2', 'IAB12-3', 'IAB13', 'IAB14', 'IAB14-1', 'IAB14-3', 'IAB15', 'IAB15-10', 'IAB16', 'IAB17', 'IAB17-1', 'IAB17-12', 'IAB17-39', 'IAB17-44', 'IAB18', 'IAB19', 'IAB19-1', 'IAB19-10', 'IAB19-11', 'IAB19-12', 'IAB19-13', 'IAB19-14', 'IAB19-15', 'IAB19-16', 'IAB19-17', 'IAB19-18', 'IAB19-19', 'IAB19-2', 'IAB19-20', 'IAB19-21', 'IAB19-22', 'IAB19-23', 'IAB19-24', 'IAB19-25', 'IAB19-26', 'IAB19-27', 'IAB19-28', 'IAB19-29', 'IAB19-3', 'IAB19-30', 'IAB19-31', 'IAB19-32', 'IAB19-33', 'IAB19-34', 'IAB19-35', 'IAB19-36', 'IAB19-4', 'IAB19-5', 'IAB19-6', 'IAB19-7', 'IAB19-8', 'IAB2', 'IAB20', 'IAB20-3', 'IAB21', 'IAB22', 'IAB24', 'IAB3', 'IAB5', 'IAB5-8', 'IAB6', 'IAB6-4', 'IAB6-5', 'IAB7', 'IAB7-1', 'IAB7-10', 'IAB7-11', 'IAB7-12', 'IAB7-13', 'IAB7-14', 'IAB7-15', 'IAB7-16', 'IAB7-17', 'IAB7-18', 'IAB7-19', 'IAB7-2', 'IAB7-20', 'IAB7-21', 'IAB7-22', 'IAB7-23', 'IAB7-24', 'IAB7-25', 'IAB7-26', 'IAB7-27', 'IAB7-28', 'IAB7-29', 'IAB7-3', 'IAB7-30', 'IAB7-31', 'IAB7-32', 'IAB7-33', 'IAB7-34', 'IAB7-35', 'IAB7-36', 'IAB7-37', 'IAB7-38', 'IAB7-39', 'IAB7-4', 'IAB7-40', 'IAB7-41', 'IAB7-42', 'IAB7-43', 'IAB7-44', 'IAB7-45', 'IAB7-5', 'IAB7-6', 'IAB7-7', 'IAB7-8', 'IAB7-9', 'IAB8', 'IAB9', 'IAB9-1', 'IAB9-11', 'IAB9-13', 'IAB9-2', 'IAB9-23', 'IAB9-24', 'IAB9-25', 'IAB9-26', 'IAB9-30', 'IAB9-5', 'IAB9-7', 'IAB9-8', 'books', 'business', 'education', 'entertainment', 'finance', 'games', 'healthcare_and_fitness', 'lifestyle', 'mañana', 'medical', 'music', 'navigation', 'news', 'noche', 'photography', 'productivity', 'reference', 'social_networking', 'sports', 'tarde', 'travel', 'utilities', 'weather'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                              # 20% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Asegurarse de que X_train y X_val sean dataframes de pandas\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_val = pd.DataFrame(X_val)\n",
    "\n",
    "# Asegurarse de que y_train y y_val sean series de pandas\n",
    "y_train = pd.Series(y_train)\n",
    "y_val = pd.Series(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = [col for col in X_train.select_dtypes(include=['object']).columns if col not in categories]\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Crear el preprocesador para manejar NaNs y aplicar One-Hot Encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media y no modificar las demás columnas\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='passthrough'  # Mantener el resto de las columnas como están\n",
    ")\n",
    "\n",
    "# Aplicar el pipeline de preprocesamiento a los datos de entrenamiento\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "# Aplicar el pipeline al conjunto de validación\n",
    "X_val_encoded = preprocessor.transform(X_val)\n",
    "\n",
    "# Convertir los resultados a DataFrame si es necesario\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=preprocessor.get_feature_names_out())\n",
    "X_val_encoded = pd.DataFrame(X_val_encoded, columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "# Asegurarse de que y_train y y_val sean series de pandas\n",
    "y_train = pd.Series(y_train)\n",
    "y_val = pd.Series(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Cargar los datos de testeo\n",
    "test_data = pd.read_csv(\"data/ctr_test.csv\")\n",
    "\n",
    "# Paso 2: Modificar la columna auction_list_0 (expansión de la lista en múltiples columnas)\n",
    "categories2, test_data = data_cleaning(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Ajustar las columnas del conjunto de testeo\n",
    "test_data = ajustar_columnas_test(X_train, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Aplicar el preprocesador al conjunto de testeo\n",
    "test_data_encoded = preprocessor.transform(test_data)\n",
    "\n",
    "# Convertir los resultados a DataFrame si es necesario\n",
    "test_data_encoded = pd.DataFrame(test_data_encoded, columns=preprocessor.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_encoded.to_csv('test_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_encoded = pd.read_csv('test_data_cleaned.csv')\n",
    "\n",
    "test_data = pd.read_csv(\"data/ctr_test.csv\")\n",
    "\n",
    "print(f\"Columnas en el conjunto de entrenamiento: {X_train_encoded.shape[1]}\")\n",
    "print(f\"Columnas en el conjunto de testeo: {test_data_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busqueda de hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'max_samples': np.linspace(0.5, 1.0, num=10),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Definir el modelo\n",
    "model = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "# Definir el RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=2,  # Número de combinaciones de hiperparámetros a probar\n",
    "    scoring='roc_auc',\n",
    "    cv=3,  # Número de folds en la validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "random_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validación con el mejor modelo encontrado\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "# Calcular el AUC en el conjunto de validación\n",
    "auc = roc_auc_score(y_val, y_pred_proba)\n",
    "print(f\"AUC en el conjunto de validación: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [None, 10, 20, 30, 40, 50]),\n",
    "    'max_samples': hp.uniform('max_samples', 0.5, 1.0),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),\n",
    "    'bootstrap': True\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt\n",
    "def objective(params):\n",
    "    # Asegurarse de que 'max_samples' solo se usa si 'bootstrap' es True\n",
    "    if not params['bootstrap']:\n",
    "        params['max_samples'] = None  # No se puede utilizar max_samples si bootstrap es False\n",
    "    \n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model = RandomForestClassifier(**params, random_state=random_state)\n",
    "    \n",
    "    # Entrenar el modelo directamente ya que el Target Encoder e Imputador ya se aplicaron\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    # Hacer predicciones en el conjunto de validación\n",
    "    y_pred_proba = model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "    # Calcular el AUC en el conjunto de validación\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "\n",
    "    # Retornar el valor negativo del AUC ya que Hyperopt minimiza por defecto\n",
    "    return {'loss': 1-auc, 'status': STATUS_OK}\n",
    "\n",
    "# Ejecutar la optimización\n",
    "trials = Trials()  # Para guardar información sobre cada iteración\n",
    "best = fmin(fn=objective, \n",
    "            space=space, \n",
    "            algo=tpe.suggest, \n",
    "            max_evals=10,  # Número de evaluaciones\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros para XGBoost\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 5, 10, 15, 20],  # Profundidad máxima del árbol\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),  # Tasa de aprendizaje\n",
    "    'subsample': np.linspace(0.5, 1.0, num=5),  # Fracción de muestras a usar para cada árbol\n",
    "    'colsample_bytree': np.linspace(0.5, 1.0, num=5),  # Fracción de características a usar para cada árbol\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],  # Regularización\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],  # Regularización L1\n",
    "    'reg_lambda': [1, 1.5, 2, 3]  # Regularización L2\n",
    "}\n",
    "\n",
    "# Definir el modelo\n",
    "model = XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Definir el RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=2,  # Número de combinaciones de hiperparámetros a probar\n",
    "    scoring='roc_auc',  # Métrica para evaluar\n",
    "    cv=3,  # Número de folds en la validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "random_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validación con el mejor modelo encontrado\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "# Calcular el AUC en el conjunto de validación\n",
    "auc = roc_auc_score(y_val, y_pred_proba)\n",
    "print(f\"AUC en el conjunto de validación: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espacio de búsqueda para los hiperparámetros\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 1, 10),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2)\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt\n",
    "def objective(params):\n",
    "    # Entrenar el modelo con los hiperparámetros actuales\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=int(params['max_depth']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Entrenar el modelo con los datos de entrenamiento\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "    \n",
    "    # Predecir las probabilidades para calcular el AUC en el conjunto de validación\n",
    "    y_pred_proba = model.predict_proba(X_val_encoded)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10,  # Número de evaluaciones\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reentrenar el modelo con los mejores hiperparámetros\n",
    "best_model = xgb.XGBClassifier(\n",
    "    max_depth=int(best['max_depth']),\n",
    "    learning_rate=best['learning_rate'],\n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    gamma=best['gamma'],\n",
    "    min_child_weight=best['min_child_weight'],\n",
    "    subsample=best['subsample'],\n",
    "    colsample_bytree=best['colsample_bytree'],\n",
    "    scale_pos_weight=best['scale_pos_weight'],\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros en los datos completos\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de validación\n",
    "y_pred_proba_val = best_model.predict_proba(X_val)[:, 1]\n",
    "auc_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "print(f\"AUC en el conjunto de validación: {auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pueba de Hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_samples': 0.6666666666666666, 'max_depth': 50, 'bootstrap': True}\n",
    "# Reconstruir el modelo con los mejores hiperparámetros\n",
    "best_model_rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=20,\n",
    "    max_samples=None,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=4,\n",
    "    bootstrap=False,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "rf_pipeline = make_pipeline(SimpleImputer(), best_model_rf)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "rf_pipeline.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_rf_test = rf_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_rf = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_rf_test})\n",
    "submission_df_rf[\"id\"] = submission_df_rf[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"random_forest\"\n",
    "file_name_rf = (\n",
    "    f\"random_forest_predictions_n_estimators_{500}_\"\n",
    "    f\"max_depth_{20}_\"\n",
    "    f\"min_samples_split_{5}_\"\n",
    "    f\"min_samples_leaf_{4}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_rf.to_csv(os.path.join(\"submits\", file_name_rf), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los hiperparámetros a los tipos correctos\n",
    "random_search.best_params_['n_estimators'] = int(random_search.best_params_['n_estimators'])\n",
    "random_search.best_params_['min_samples_split'] = int(random_search.best_params_['min_samples_split'])\n",
    "random_search.best_params_['min_samples_leaf'] = int(random_search.best_params_['min_samples_leaf'])\n",
    "random_search.best_params_['max_depth'] = int(random_search.best_params_['max_depth']) if random_search.best_params_['max_depth'] is not None else None\n",
    "\n",
    "# Reconstruir el modelo con los mejores hiperparámetros\n",
    "best_model_rf = RandomForestClassifier(\n",
    "    n_estimators=random_search.best_params_['n_estimators'],\n",
    "    max_depth=random_search.best_params_['max_depth'],  # Asegurarse de que max_depth sea un entero o None\n",
    "    min_samples_split=random_search.best_params_['min_samples_split'],\n",
    "    min_samples_leaf=random_search.best_params_['min_samples_leaf'],\n",
    "    bootstrap=random_search.best_params_['bootstrap'],\n",
    "    random_state=random_state,\n",
    "    max_samples=random_search.best_params_['max_samples']\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "rf_pipeline = make_pipeline(SimpleImputer(), best_model_rf)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "rf_pipeline.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_rf_test = rf_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_rf = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_rf_test})\n",
    "submission_df_rf[\"id\"] = submission_df_rf[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"random_forest\"\n",
    "file_name_rf = (\n",
    "    f\"random_forest_predictions_n_estimators_{random_search.best_params_['n_estimators']}_\"\n",
    "    f\"max_depth_{random_search.best_params_['max_depth']}_\"\n",
    "    f\"min_samples_split_{random_search.best_params_['min_samples_split']}_\"\n",
    "    f\"max_samples_{random_search.best_params_['max_samples']}_\"\n",
    "    f\"bootstrap_{random_search.best_params_['bootstrap']}_\"\n",
    "    f\"min_samples_leaf_{random_search.best_params_['min_samples_leaf']}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_rf.to_csv(os.path.join(\"submits\", file_name_rf), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.016434700376417533,\n",
    "    subsample=0.6696027253148403,\n",
    "    scale_pos_weight=1.9885767576831004,\n",
    "    colsample_bytree=0.6189520886008961,\n",
    "    min_child_weight=6.378545393104531,\n",
    "    gamma=0.05263130692482787,\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Si estás usando una versión reciente de XGBoost\n",
    "    eval_metric=\"auc\"  # Ajustar el métrico de evaluación\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "xgb_pipeline = make_pipeline(SimpleImputer(), best_model_xgb)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"xgboost\"\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_predictions_n_estimators_{500}_\"\n",
    "    f\"max_depth_{20}_\"\n",
    "    f\"learning_rate_{0.05}_\"\n",
    "    f\"min_child_weight_{4}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atomatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los hiperparámetros a enteros si es necesario\n",
    "best['n_estimators'] = int(best['n_estimators'])\n",
    "best['max_depth'] = [None, 10, 20, 30, 40, 50][best['max_depth']]  # Ajustar el valor del índice si es necesario\n",
    "best['min_child_weight'] = int(best['min_child_weight'])\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=best['n_estimators'],\n",
    "    max_depth=best['max_depth'],\n",
    "    learning_rate=best['learning_rate'],\n",
    "    subsample=best['subsample'],\n",
    "    colsample_bytree=best['colsample_bytree'],\n",
    "    min_child_weight=best['min_child_weight'],\n",
    "    gamma=best['gamma'],\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "xgb_pipeline = make_pipeline(SimpleImputer(), best_model_xgb)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"xgboost\"\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_predictions_n_estimators_{best['n_estimators']}_\"\n",
    "    f\"max_depth_{best['max_depth']}_\"\n",
    "    f\"learning_rate_{best['learning_rate']}_\"\n",
    "    f\"subsample_{best['subsample']}_\"\n",
    "    f\"colsample_bytree_{best['colsample_bytree']}_\"\n",
    "    f\"gamma_{best['gamma']}_\"\n",
    "    f\"min_child_weight_{best['min_child_weight']}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
