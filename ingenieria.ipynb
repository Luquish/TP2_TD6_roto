{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrupar_categorias_pequenas(df, columna, umbral=100):\n",
    "    # Contar la frecuencia de cada categoría\n",
    "    conteo_categorias = df[columna].value_counts()\n",
    "    \n",
    "    # Identificar categorías con observaciones por debajo del umbral\n",
    "    categorias_pequenas = conteo_categorias[conteo_categorias < umbral].index\n",
    "    \n",
    "    # Mantener el mismo tipo de las categorías originales\n",
    "    tipo_original = df[columna].dtype.type\n",
    "    \n",
    "    # Reemplazar esas categorías con 'Otros', asegurando el mismo tipo de dato\n",
    "    df[columna] = df[columna].apply(lambda x: tipo_original('Otros') if x in categorias_pequenas else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    if min(r, k) == 1:\n",
    "        return np.nan  # Evita dividir por cero\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "def expand_list(df, column):\n",
    "    # Crear un conjunto de categorías\n",
    "    categories = set()\n",
    "    \n",
    "    # Recorrer todas las listas y agregar cada categoría única al conjunto\n",
    "    for row in df[column]:\n",
    "        if pd.notna(row):\n",
    "            # Verificar si el valor es una lista; si es string, se convierte en lista\n",
    "            if isinstance(row, str):\n",
    "                try:\n",
    "                    row_list = eval(row)  # Transforma la string en lista\n",
    "                except:\n",
    "                    row_list = [row]  # Si no es lista, lo trata como valor único\n",
    "            else:\n",
    "                row_list = row if isinstance(row, list) else [row]\n",
    "            \n",
    "            # Añadir los elementos de la lista a las categorías\n",
    "            categories.update(map(str, row_list))  # Convertir todos los elementos a string\n",
    "\n",
    "    # Crear un DataFrame temporal para almacenar las nuevas columnas\n",
    "    new_columns = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Llenar el DataFrame temporal con las nuevas columnas\n",
    "    for category in categories:\n",
    "        # Crear el nombre de la nueva columna con el sufijo del nombre de la columna original\n",
    "        new_column_name = f\"{category}_{column}\"\n",
    "        new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
    "\n",
    "    # Concatenar el nuevo DataFrame con las columnas originales\n",
    "    df = pd.concat([df.drop(columns=[column]), new_columns], axis=1)\n",
    "\n",
    "    return categories, df\n",
    "\n",
    "\n",
    "def categorizar_hora(hora):\n",
    "        hora = int(hora.split(':')[0])\n",
    "        if 0 <= hora < 12:\n",
    "            return 'mañana'\n",
    "        elif 12 <= hora < 18:\n",
    "            return 'tarde'\n",
    "        else:\n",
    "            return 'noche'\n",
    "\n",
    "def convertir_auction_time(df):\n",
    "    \n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s').dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    # Crear una nueva columna con las categorías de tiempo\n",
    "    df['parte_del_dia'] = df['auction_time'].apply(categorizar_hora)\n",
    "    \n",
    "    # Crear columnas binarias para cada parte del día\n",
    "    df['mañana'] = (df['parte_del_dia'] == 'mañana').astype(int)\n",
    "    df['tarde'] = (df['parte_del_dia'] == 'tarde').astype(int)\n",
    "    df['noche'] = (df['parte_del_dia'] == 'noche').astype(int)\n",
    "    \n",
    "    # Opcional: Eliminar la columna 'parte_del_dia' si no se necesita\n",
    "    df.drop(['parte_del_dia', 'auction_time'], axis=1, inplace=True)\n",
    "\n",
    "    return ['mañana', 'tarde', 'noche'], df\n",
    "\n",
    "def imputar_nan_como_otros(df):\n",
    "    for columna in df.columns:\n",
    "        if df[columna].isnull().any():  # Verificar si hay NaN en la columna\n",
    "            tipo_original = df[columna].dtype  # Guardar el tipo original de la columna\n",
    "            # Imputar 'Otros' y asegurarse de que el tipo de dato se mantiene\n",
    "            df[columna] = df[columna].fillna(tipo_original.type('Otros'))\n",
    "    return df\n",
    "\n",
    "def data_cleaning(df):\n",
    "    all_categories = set()\n",
    "\n",
    "    categorical_features = [col for col in df.select_dtypes(include=['object']).columns.tolist() if col not in ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']]\n",
    "\n",
    "    categories, df = convertir_auction_time(df)\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    categories, df = expand_list(df, 'auction_list_0')\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    categories, df = expand_list(df, 'action_list_1')\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    categories, df = expand_list(df, 'action_list_2')\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    for col in categorical_features:\n",
    "        df = agrupar_categorias_pequenas(df, col, umbral=100) \n",
    "\n",
    "    return all_categories, df\n",
    "\n",
    "def ajustar_columnas_test(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Asegura que las columnas del conjunto de test coincidan con las del conjunto de entrenamiento,\n",
    "    con la excepción de que 'Label' no se puede borrar de train y 'id' no se puede borrar de test.\n",
    "    \n",
    "    Si faltan columnas en el conjunto de test, se agregan con valor 0.\n",
    "    Si sobran columnas en el conjunto de test que no están en el de entrenamiento, se eliminan.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df: DataFrame del conjunto de entrenamiento (con las columnas transformadas)\n",
    "    - test_df: DataFrame del conjunto de test (sin transformar)\n",
    "    \n",
    "    Returns:\n",
    "    - test_df: DataFrame del conjunto de test con las columnas ajustadas.\n",
    "    \"\"\"\n",
    "    # Encontrar las columnas que están en train pero no en test, excepto 'Label'\n",
    "    missing_cols = set(train_df.columns) - set(test_df.columns) - {'Label'}\n",
    "\n",
    "    # Encontrar las columnas que están en test pero no en train, excepto 'id'\n",
    "    extra_cols = set(test_df.columns) - set(train_df.columns) - {'id'}\n",
    "\n",
    "    # Agregar las columnas faltantes en el conjunto de test con valor 0\n",
    "    for col in missing_cols:\n",
    "        test_df[col] = 0\n",
    "\n",
    "    # Eliminar las columnas extra del conjunto de test, excepto 'id'\n",
    "    test_df = test_df.drop(columns=extra_cols)\n",
    "\n",
    "    # Asegurarse de que las columnas estén en el mismo orden que en el conjunto de entrenamiento\n",
    "    test_df = test_df[train_df.columns.difference(['Label']).tolist() + ['id']]\n",
    "\n",
    "    return test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_21 = pd.read_csv(\"data/ctr_21.csv\")\n",
    "# train_data_20 = pd.read_csv(\"data/ctr_20.csv\")\n",
    "# train_data_19 = pd.read_csv(\"data/ctr_19.csv\")\n",
    "# train_data_18 = pd.read_csv(\"data/ctr_18.csv\")\n",
    "# train_data_17 = pd.read_csv(\"data/ctr_17.csv\")\n",
    "# train_data_16 = pd.read_csv(\"data/ctr_16.csv\")\n",
    "# train_data_15 = pd.read_csv(\"data/ctr_15.csv\")\n",
    "# train_data_combined = pd.concat([train_data_21, train_data_20, train_data_19, train_data_18, train_data_17, train_data_16, train_data_15], axis=0)\n",
    "# train_data_combined = pd.concat([train_data_21, train_data_20], axis=0)\n",
    "train_data_combined = pd.read_csv(\"data/ctr_21.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43992294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    0.98567\n",
      "1    0.01433\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Reducir el conjunto de datos mientras se mantiene la distribución de 'label'\n",
    "train_data_combined, _ = train_test_split(train_data_combined, train_size=100000, stratify=train_data_combined['Label'], random_state=42)\n",
    "\n",
    "# Verificar la distribución en el conjunto reducido\n",
    "print(train_data_combined['Label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caracteristicas principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la cantidad de filas del dataset combinado\n",
    "print(f\"Cantidad de filas en el dataset combinado: {train_data_combined.shape[0]}\")\n",
    "\n",
    "# Ver porcentaje de clics vs no clics en la columna Label\n",
    "label_counts = train_data_combined['Label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPorcentaje de clics (1) y no clics (0):\")\n",
    "print(label_counts)\n",
    "\n",
    "# Cantidad de clics (1) y no clics (0)\n",
    "label_counts_abs = train_data_combined['Label'].value_counts()\n",
    "print(\"\\nCantidad de clics (1) y no clics (0):\")\n",
    "print(label_counts_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico de las características numéricas\n",
    "print(train_data_combined.describe())\n",
    "\n",
    "# Resumen de las características categóricas\n",
    "print(train_data_combined.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el porcentaje de valores faltantes por columna\n",
    "missing_data = train_data_combined.isnull().mean() * 100\n",
    "print(missing_data[missing_data > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en numéricos y categóricos\n",
    "numeric_features = train_data_combined.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "categorical_features = train_data_combined.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columna por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos\n",
    "for column in train_data_combined.columns:\n",
    "    unique_values = train_data_combined[column].unique()\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos y la cantidad de veces que aparecen\n",
    "for column in train_data_combined.columns:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(train_data_combined[column].value_counts())\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7265, -7264, -7263, -7196, -7195, -7190, -7143, -7127, -7126, -7112, -7111, -6946, -6938, -6929, -6905, -6904, -6876, -6875, -6871, -6867, -6866, -6865, -6850, -6849, -6848, -6823, -6800, -6780, -6775, -6774, -6770, -6621, -6620, -6618, -6617, -6615, -6548, -6547, -6544, -6543, -6454, -6309, -6226, -6224, -6223, -6220, -6219, -6218, -6217, -6125, -6119, -6118, -5902, -5613, -5605, -5604, -5603, -5579, -5578, -5577, -5576, -5560, -5559, -5471, -5470, -5469, -2560, 2560, 5469, 5470, 5471, 5559, 5560, 5576, 5577, 5578, 5579, 5603, 5604, 5605, 5613, 5902, 6118, 6119, 6125, 6217, 6218, 6219, 6220, 6223, 6224, 6226, 6309, 6451, 6454, 6543, 6544, 6547, 6548, 6615, 6616, 6618, 6620, 6621, 6770, 6774, 6775, 6780, 6800, 6823, 6848, 6849, 6850, 6865, 6866, 6867, 6871, 6875, 6876, 6904, 6905, 6929, 6938, 6946, 7111, 7112, 7126, 7127, 7143, 7190, 7195, 7196, 7263, 7264, 7265]\n"
     ]
    }
   ],
   "source": [
    "# Inicializar un conjunto vacío para almacenar los entity ids únicos\n",
    "unique_entity_ids_2 = set()\n",
    "\n",
    "# Recorrer cada fila de la columna 'action_list_2'\n",
    "for lista in train_data_combined['action_list_2'].dropna():\n",
    "    # Convertir el string de la lista a una lista real y agregar cada valor al conjunto\n",
    "    unique_entity_ids_2.update(eval(lista))  # eval transforma la string en lista si es necesario\n",
    "\n",
    "# Convertir el conjunto en una lista ordenada\n",
    "unique_entity_ids_2 = sorted(unique_entity_ids_2)\n",
    "\n",
    "# Mostrar los entity ids únicos\n",
    "print(unique_entity_ids_2)\n",
    "print(len(unique_entity_ids_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7199, -7195, -7194, -7190, -7033, -6903, -6902, -6874, -6871, -6824, -6823, -6780, -6779, -6618, -6617, -6616, -6615, -6614, -6613, -6454, -6451, -6119, -6118, -5736, -5579, -5578, -5577, -5559, -2606, 2606, 5559, 5577, 5578, 5579, 5736, 6118, 6119, 6451, 6454, 6614, 6615, 6616, 6617, 6618, 6779, 6780, 6823, 6824, 6871, 6874, 6902, 6903, 7033, 7190, 7194, 7195, 7199]\n"
     ]
    }
   ],
   "source": [
    "# Inicializar un conjunto vacío para almacenar los entity ids únicos\n",
    "unique_entity_ids = set()\n",
    "\n",
    "# Recorrer cada fila de la columna 'action_list_1'\n",
    "for lista in train_data_combined['action_list_1'].dropna():\n",
    "    # Convertir el string de la lista a una lista real y agregar cada valor al conjunto\n",
    "    unique_entity_ids.update(eval(lista))  # eval transforma la string en lista si es necesario\n",
    "\n",
    "# Convertir el conjunto en una lista ordenada\n",
    "unique_entity_ids = sorted(unique_entity_ids)\n",
    "\n",
    "# Mostrar los entity ids únicos\n",
    "print(unique_entity_ids)\n",
    "print(len(unique_entity_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creative_categorical_3     1453098\n",
      "creative_categorical_6     1261556\n",
      "creative_categorical_7     1256292\n",
      "creative_categorical_12    1253433\n",
      "auction_age                1230041\n",
      "creative_categorical_5     1205582\n",
      "creative_categorical_2     1197706\n",
      "gender                     1175827\n",
      "action_list_1               549228\n",
      "auction_categorical_9       415685\n",
      "action_list_2               412296\n",
      "creative_categorical_4      317639\n",
      "creative_width              279643\n",
      "creative_height             279643\n",
      "auction_categorical_6       119775\n",
      "auction_boolean_1            44370\n",
      "auction_boolean_2            39803\n",
      "auction_boolean_0            22728\n",
      "auction_list_0               22574\n",
      "creative_categorical_9       13317\n",
      "auction_categorical_2        12693\n",
      "auction_categorical_0         7034\n",
      "auction_categorical_4         5997\n",
      "timezone_offset               4885\n",
      "auction_categorical_3         4870\n",
      "auction_categorical_11        2396\n",
      "auction_categorical_7           43\n",
      "auction_categorical_12           2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calcular la cantidad de valores NaN en cada columna\n",
    "nan_counts = train_data_combined.isna().sum()\n",
    "\n",
    "# Filtrar columnas que tienen al menos un NaN\n",
    "nan_counts_filtered = nan_counts[nan_counts > 0]\n",
    "\n",
    "# Mostrar las columnas con valores NaN de mayor a menor\n",
    "print(nan_counts_filtered.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos\n",
    "for column in train_data_combined.columns:\n",
    "    unique_values = train_data_combined[column].unique()\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n"
     ]
    }
   ],
   "source": [
    "categories, train_data_combined = data_cleaning(train_data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el dataset modificado como CSV antes de aplicar One-Hot Encoding\n",
    "train_data_combined.to_csv('train_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos\n",
    "for column in train_data_combined.columns:\n",
    "    unique_values = train_data_combined[column].unique()  # Obtener los valores únicos para cada columna\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined.isnull().sum()[train_data_combined.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                              # 20% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Crear el Target Encoder para las columnas categóricas\n",
    "target_encoder = ce.TargetEncoder(cols=categorical_features)\n",
    "\n",
    "# Ajustar y transformar los datos de entrenamiento\n",
    "X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transformar los datos de validación\n",
    "X_val_encoded = target_encoder.transform(X_val)\n",
    "\n",
    "# Crear un imputador que llene los valores NaN con la media de cada columna\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Aplicar el imputador a los datos de entrenamiento\n",
    "X_train_encoded = imputer.fit_transform(X_train_encoded)\n",
    "\n",
    "# También aplica el imputador al conjunto de validación\n",
    "X_val_encoded = imputer.transform(X_val_encoded)\n",
    "\n",
    "# Convertir los resultados a DataFrame y asignar las columnas originales\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=X_train.columns)\n",
    "X_val_encoded = pd.DataFrame(X_val_encoded, columns=X_val.columns)\n",
    "\n",
    "# Ver los primeros datos\n",
    "print(X_train_encoded.head())\n",
    "print(X_val_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de testeo\n",
    "test_data = pd.read_csv(\"data/ctr_test.csv\")\n",
    "\n",
    "# Eliminar las columnas no deseadas\n",
    "test_data = test_data.drop(columns=columns_to_drop)\n",
    "\n",
    "test_data_encoded = target_encoder.transform(test_data.drop(columns=[\"id\"]))\n",
    "test_data_encoded = imputer.transform(test_data_encoded)  # Imputar los valores faltantes en el set de testeo\n",
    "\n",
    "# Convertir los resultados a DataFrame y asignar las columnas originales\n",
    "test_data_encoded = pd.DataFrame(test_data_encoded, columns=test_data.drop(columns=[\"id\"]).columns)\n",
    "\n",
    "# comparar tamaño de columnas\n",
    "print(X_train_encoded.columns)\n",
    "print(test_data_encoded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined = pd.read_csv('train_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                              # 20% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Asegurarse de que X_train y X_val sean dataframes de pandas\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_val = pd.DataFrame(X_val)\n",
    "\n",
    "# Asegurarse de que y_train y y_val sean series de pandas\n",
    "y_train = pd.Series(y_train)\n",
    "y_val = pd.Series(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = [col for col in X_train.select_dtypes(include=['object']).columns if col not in categories]\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Crear el preprocesador para manejar NaNs y aplicar One-Hot Encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media y no modificar las demás columnas\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='passthrough'  # Mantener el resto de las columnas como están\n",
    ")\n",
    "\n",
    "# Aplicar el pipeline de preprocesamiento a los datos de entrenamiento\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "# Aplicar el pipeline al conjunto de validación\n",
    "X_val_encoded = preprocessor.transform(X_val)\n",
    "\n",
    "# Convertir los resultados a DataFrame si es necesario\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=preprocessor.get_feature_names_out())\n",
    "X_val_encoded = pd.DataFrame(X_val_encoded, columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "# Asegurarse de que y_train y y_val sean series de pandas\n",
    "y_train = pd.Series(y_train)\n",
    "y_val = pd.Series(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n",
      "/var/folders/5k/nl_6tbmj7mx91g71vzpwrch40000gn/T/ipykernel_67079/2714462713.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_columns[new_column_name] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category) in str(x) else 0)\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Cargar los datos de testeo\n",
    "test_data = pd.read_csv(\"data/ctr_test.csv\")\n",
    "\n",
    "# Paso 2: Modificar la columna auction_list_0 (expansión de la lista en múltiples columnas)\n",
    "categories, test_data = data_cleaning(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:227: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 4, 7, 8, 10, 12, 15, 16, 18, 21, 22, 24, 25, 29, 30, 31, 33, 34, 35, 38] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en el conjunto de entrenamiento: 1471\n",
      "Columnas en el conjunto de testeo: 1471\n"
     ]
    }
   ],
   "source": [
    "# Paso 3: Ajustar las columnas del conjunto de testeo\n",
    "test_data = ajustar_columnas_test(X_train, test_data)\n",
    "\n",
    "# Paso 4: Aplicar el preprocesador al conjunto de testeo\n",
    "test_data_encoded = preprocessor.transform(test_data)\n",
    "\n",
    "# Convertir los resultados a DataFrame si es necesario\n",
    "test_data_encoded = pd.DataFrame(test_data_encoded, columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "print(f\"Columnas en el conjunto de entrenamiento: {X_train_encoded.shape[1]}\")\n",
    "print(f\"Columnas en el conjunto de testeo: {test_data_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busqueda de hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'max_samples': np.linspace(0.5, 1.0, num=10),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Definir el modelo\n",
    "model = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "# Definir el RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Número de combinaciones de hiperparámetros a probar\n",
    "    scoring='roc_auc',\n",
    "    cv=3,  # Número de folds en la validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "random_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validación con el mejor modelo encontrado\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "# Calcular el AUC en el conjunto de validación\n",
    "auc = roc_auc_score(y_val, y_pred_proba)\n",
    "print(f\"AUC en el conjunto de validación: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [None, 10, 20, 30, 40, 50]),\n",
    "    'max_samples': hp.uniform('max_samples', 0.5, 1.0),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),\n",
    "    'bootstrap': True\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt\n",
    "def objective(params):\n",
    "    # Asegurarse de que 'max_samples' solo se usa si 'bootstrap' es True\n",
    "    if not params['bootstrap']:\n",
    "        params['max_samples'] = None  # No se puede utilizar max_samples si bootstrap es False\n",
    "    \n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model = RandomForestClassifier(**params, random_state=random_state)\n",
    "    \n",
    "    # Entrenar el modelo directamente ya que el Target Encoder e Imputador ya se aplicaron\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    # Hacer predicciones en el conjunto de validación\n",
    "    y_pred_proba = model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "    # Calcular el AUC en el conjunto de validación\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "\n",
    "    # Retornar el valor negativo del AUC ya que Hyperopt minimiza por defecto\n",
    "    return {'loss': 1-auc, 'status': STATUS_OK}\n",
    "\n",
    "# Ejecutar la optimización\n",
    "trials = Trials()  # Para guardar información sobre cada iteración\n",
    "best = fmin(fn=objective, \n",
    "            space=space, \n",
    "            algo=tpe.suggest, \n",
    "            max_evals=50,  # Número de evaluaciones\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiperparámetros: {'colsample_bytree': 0.6189520886008961, 'gamma': 0.05263130692482787, 'learning_rate': 0.016434700376417533, 'max_depth': 3, 'min_child_weight': 6.378545393104531, 'n_estimators': 200, 'scale_pos_weight': 1.9885767576831004, 'subsample': 0.6696027253148403}, AUC: 0.8376\n",
      "  2%|▏         | 1/50 [01:58<1:36:47, 118.51s/trial, best loss: -0.8375552417610834]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "\n",
      "/Users/lucamazzarello_/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [12:15<10:00:48, 735.69s/trial, best loss: -0.8375552417610834]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Ejecutar la optimización\u001b[39;00m\n\u001b[1;32m     43\u001b[0m trials \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[0;32m---> 44\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Número de evaluaciones\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMejores hiperparámetros encontrados:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(best)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\n\u001b[1;32m     17\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     18\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mrandom_state\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo con los datos de entrenamiento\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Predecir las probabilidades para calcular el AUC en el conjunto de validación\u001b[39;00m\n\u001b[1;32m     34\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val_encoded)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1462\u001b[0m (\n\u001b[1;32m   1463\u001b[0m     model,\n\u001b[1;32m   1464\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1470\u001b[0m )\n\u001b[1;32m   1471\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1472\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1473\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1488\u001b[0m )\n\u001b[0;32m-> 1490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Espacio de búsqueda para los hiperparámetros\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 1, 10),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2)\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt\n",
    "def objective(params):\n",
    "    # Entrenar el modelo con los hiperparámetros actuales\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=int(params['max_depth']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Entrenar el modelo con los datos de entrenamiento\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "    \n",
    "    # Predecir las probabilidades para calcular el AUC en el conjunto de validación\n",
    "    y_pred_proba = model.predict_proba(X_val_encoded)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,  # Número de evaluaciones\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reentrenar el modelo con los mejores hiperparámetros\n",
    "best_model = xgb.XGBClassifier(\n",
    "    max_depth=int(best['max_depth']),\n",
    "    learning_rate=best['learning_rate'],\n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    gamma=best['gamma'],\n",
    "    min_child_weight=best['min_child_weight'],\n",
    "    subsample=best['subsample'],\n",
    "    colsample_bytree=best['colsample_bytree'],\n",
    "    scale_pos_weight=best['scale_pos_weight'],\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros en los datos completos\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de validación\n",
    "y_pred_proba_val = best_model.predict_proba(X_val)[:, 1]\n",
    "auc_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "print(f\"AUC en el conjunto de validación: {auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pueba de Hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir el modelo con los mejores hiperparámetros\n",
    "best_model_rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=20,\n",
    "    max_samples=None,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=4,\n",
    "    bootstrap=False,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "rf_pipeline = make_pipeline(SimpleImputer(), best_model_rf)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "rf_pipeline.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_rf_test = rf_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_rf = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_rf_test})\n",
    "submission_df_rf[\"id\"] = submission_df_rf[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"random_forest\"\n",
    "file_name_rf = (\n",
    "    f\"random_forest_predictions_n_estimators_{500}_\"\n",
    "    f\"max_depth_{20}_\"\n",
    "    f\"min_samples_split_{5}_\"\n",
    "    f\"min_samples_leaf_{4}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_rf.to_csv(os.path.join(\"submits\", file_name_rf), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los hiperparámetros a enteros\n",
    "best['n_estimators'] = int(best['n_estimators'])\n",
    "best['min_samples_split'] = int(best['min_samples_split'])\n",
    "best['min_samples_leaf'] = int(best['min_samples_leaf'])\n",
    "\n",
    "# Reconstruir el modelo con los mejores hiperparámetros\n",
    "best_model_rf = RandomForestClassifier(\n",
    "    n_estimators=best['n_estimators'],\n",
    "    max_depth=[None, 10, 20, 30, 40, 50][best['max_depth']],  # Hyperopt devuelve índices para opciones de lista\n",
    "    max_samples=best['max_samples'],  # Para max_samples no es un índice, es un valor continuo\n",
    "    min_samples_split=best['min_samples_split'],\n",
    "    min_samples_leaf=best['min_samples_leaf'],\n",
    "    bootstrap=[True, False][best['bootstrap']],  # Igual que max_depth\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "rf_pipeline = make_pipeline(SimpleImputer(), best_model_rf)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "rf_pipeline.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predecir en el conjunto de testeo\n",
    "test_data_encoded = target_encoder.transform(test_data.select_dtypes(include='number').drop(columns=[\"id\"]))\n",
    "test_data_encoded = imputer.transform(test_data_encoded)  # Imputar los valores faltantes en el set de testeo\n",
    "y_preds_rf_test = rf_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_rf = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_rf_test})\n",
    "submission_df_rf[\"id\"] = submission_df_rf[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"random_forest\"\n",
    "file_name_rf = (\n",
    "    f\"random_forest_predictions_n_estimators_{best['n_estimators']}_\"\n",
    "    f\"max_depth_{best['max_depth']}_\"\n",
    "    f\"min_samples_split_{best['min_samples_split']}_\"\n",
    "    f\"min_samples_leaf_{best['min_samples_leaf']}_\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_rf.to_csv(os.path.join(\"submits\", file_name_rf), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=20,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=4,\n",
    "    gamma=0.1,\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Si estás usando una versión reciente de XGBoost\n",
    "    eval_metric=\"auc\"  # Ajustar el métrico de evaluación\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "xgb_pipeline = make_pipeline(SimpleImputer(), best_model_xgb)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"xgboost\"\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_predictions_n_estimators_{500}_\"\n",
    "    f\"max_depth_{20}_\"\n",
    "    f\"learning_rate_{0.05}_\"\n",
    "    f\"min_child_weight_{4}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atomatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los hiperparámetros a enteros si es necesario\n",
    "best['n_estimators'] = int(best['n_estimators'])\n",
    "best['max_depth'] = [None, 10, 20, 30, 40, 50][best['max_depth']]  # Ajustar el valor del índice si es necesario\n",
    "best['min_child_weight'] = int(best['min_child_weight'])\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=best['n_estimators'],\n",
    "    max_depth=best['max_depth'],\n",
    "    learning_rate=best['learning_rate'],\n",
    "    subsample=best['subsample'],\n",
    "    colsample_bytree=best['colsample_bytree'],\n",
    "    min_child_weight=best['min_child_weight'],\n",
    "    gamma=best['gamma'],\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "xgb_pipeline = make_pipeline(SimpleImputer(), best_model_xgb)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predecir en el conjunto de testeo\n",
    "test_data_encoded = target_encoder.transform(test_data.select_dtypes(include='number').drop(columns=[\"id\"]))\n",
    "test_data_encoded = imputer.transform(test_data_encoded)  # Imputar los valores faltantes en el set de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"xgboost\"\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_predictions_n_estimators_{best['n_estimators']}_\"\n",
    "    f\"max_depth_{best['max_depth']}_\"\n",
    "    f\"learning_rate_{best['learning_rate']}_\"\n",
    "    f\"min_child_weight_{best['min_child_weight']}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
