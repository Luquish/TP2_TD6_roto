{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrupar_categorias_pequenas(df, columna, umbral=100):\n",
    "    # Contar la frecuencia de cada categoría\n",
    "    conteo_categorias = df[columna].value_counts()\n",
    "    \n",
    "    # Identificar categorías con observaciones por debajo del umbral\n",
    "    categorias_pequenas = conteo_categorias[conteo_categorias < umbral].index\n",
    "    \n",
    "    # Mantener el mismo tipo de las categorías originales\n",
    "    tipo_original = df[columna].dtype.type\n",
    "    \n",
    "    # Reemplazar esas categorías con 'Otros', asegurando el mismo tipo de dato\n",
    "    df[columna] = df[columna].apply(lambda x: tipo_original('Otros') if x in categorias_pequenas else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    if min(r, k) == 1:\n",
    "        return np.nan  # Evita dividir por cero\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "def expand_list(df, column):\n",
    "    # Crear un conjunto de categorías\n",
    "    categories = set()\n",
    "    \n",
    "    # Recorrer todas las listas y agregar cada categoría única al conjunto\n",
    "    for row in df[column]:\n",
    "        if pd.notna(row):\n",
    "            # Verificar si el valor es una lista; si es string, se convierte en lista\n",
    "            if isinstance(row, str):\n",
    "                try:\n",
    "                    row_list = eval(row)  # Transforma la string en lista\n",
    "                except:\n",
    "                    row_list = [row]  # Si no es lista, lo trata como valor único\n",
    "            else:\n",
    "                row_list = row if isinstance(row, list) else [row]\n",
    "            \n",
    "            # Añadir los elementos de la lista a las categorías\n",
    "            categories.update(map(lambda item: f\"{item}_{column}\", map(str, row_list)))  # Convertir todos los elementos a string con el formato correcto\n",
    "\n",
    "    # Crear un DataFrame temporal para almacenar las nuevas columnas\n",
    "    new_columns = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Llenar el DataFrame temporal con las nuevas columnas\n",
    "    for category in categories:\n",
    "        new_columns[category] = df[column].apply(lambda x: 1 if pd.notna(x) and str(category.split('_')[0]) in str(x) else 0)\n",
    "\n",
    "    # Concatenar el nuevo DataFrame con las columnas originales\n",
    "    df = pd.concat([df.drop(columns=[column]), new_columns], axis=1)\n",
    "\n",
    "    return categories, df\n",
    "\n",
    "\n",
    "def categorizar_hora(hora):\n",
    "        hora = int(hora.split(':')[0])\n",
    "        if 0 <= hora < 12:\n",
    "            return 'mañana'\n",
    "        elif 12 <= hora < 18:\n",
    "            return 'tarde'\n",
    "        else:\n",
    "            return 'noche'\n",
    "\n",
    "def convertir_auction_time(df):\n",
    "    \n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s').dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    # Crear una nueva columna con las categorías de tiempo\n",
    "    df['parte_del_dia'] = df['auction_time'].apply(categorizar_hora)\n",
    "    \n",
    "    # Crear columnas binarias para cada parte del día\n",
    "    df['mañana'] = (df['parte_del_dia'] == 'mañana').astype(int)\n",
    "    df['tarde'] = (df['parte_del_dia'] == 'tarde').astype(int)\n",
    "    df['noche'] = (df['parte_del_dia'] == 'noche').astype(int)\n",
    "    \n",
    "    # Opcional: Eliminar la columna 'parte_del_dia' si no se necesita\n",
    "    df.drop(['parte_del_dia', 'auction_time'], axis=1, inplace=True)\n",
    "\n",
    "    return ['mañana', 'tarde', 'noche'], df\n",
    "\n",
    "def imputar_nan_como_otros(df):\n",
    "    for columna in df.columns:\n",
    "        if df[columna].isnull().any():  # Verificar si hay NaN en la columna\n",
    "            tipo_original = df[columna].dtype  # Guardar el tipo original de la columna\n",
    "            # Imputar 'Otros' y asegurarse de que el tipo de dato se mantiene\n",
    "            df[columna] = df[columna].fillna(tipo_original.type('Otros'))\n",
    "    return df\n",
    "\n",
    "def data_cleaning(df):\n",
    "    all_categories = set()\n",
    "\n",
    "    categorical_features = [col for col in df.select_dtypes(include=['object']).columns.tolist() if col not in ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']]\n",
    "\n",
    "    categories, df = convertir_auction_time(df)\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    categories, df = expand_list(df, 'auction_list_0')\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    categories, df = expand_list(df, 'action_list_1')\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    categories, df = expand_list(df, 'action_list_2')\n",
    "    all_categories.update(categories)\n",
    "\n",
    "    for col in categorical_features:\n",
    "        df = agrupar_categorias_pequenas(df, col, umbral=100) \n",
    "\n",
    "    return all_categories, df\n",
    "\n",
    "def ajustar_columnas_test(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Asegura que las columnas del conjunto de test coincidan con las del conjunto de entrenamiento,\n",
    "    con la excepción de que 'Label' no se puede borrar de train y 'id' no se puede borrar de test.\n",
    "    \n",
    "    Si faltan columnas en el conjunto de test, se agregan con valor 0.\n",
    "    Si sobran columnas en el conjunto de test que no están en el de entrenamiento, se eliminan.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df: DataFrame del conjunto de entrenamiento (con las columnas transformadas)\n",
    "    - test_df: DataFrame del conjunto de test (sin transformar)\n",
    "    \n",
    "    Returns:\n",
    "    - test_df: DataFrame del conjunto de test con las columnas ajustadas.\n",
    "    \"\"\"\n",
    "    # Encontrar las columnas que están en train pero no en test, excepto 'Label'\n",
    "    missing_cols = set(train_df.columns) - set(test_df.columns) - {'Label'}\n",
    "\n",
    "    # Encontrar las columnas que están en test pero no en train, excepto 'id'\n",
    "    extra_cols = set(test_df.columns) - set(train_df.columns) - {'id'}\n",
    "\n",
    "    # Agregar las columnas faltantes en el conjunto de test con valor 0\n",
    "    for col in missing_cols:\n",
    "        test_df[col] = 0\n",
    "\n",
    "    # Eliminar las columnas extra del conjunto de test, excepto 'id'\n",
    "    test_df = test_df.drop(columns=extra_cols)\n",
    "\n",
    "    # Asegurarse de que las columnas estén en el mismo orden que en el conjunto de entrenamiento\n",
    "    test_df = test_df[train_df.columns.difference(['Label']).tolist() + ['id']]\n",
    "\n",
    "    return test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_21 = pd.read_csv(\"data/ctr_21.csv\")\n",
    "# train_data_20 = pd.read_csv(\"data/ctr_20.csv\")\n",
    "# train_data_19 = pd.read_csv(\"data/ctr_19.csv\")\n",
    "# train_data_18 = pd.read_csv(\"data/ctr_18.csv\")\n",
    "# train_data_17 = pd.read_csv(\"data/ctr_17.csv\")\n",
    "# train_data_16 = pd.read_csv(\"data/ctr_16.csv\")\n",
    "# train_data_15 = pd.read_csv(\"data/ctr_15.csv\")\n",
    "# train_data_combined = pd.concat([train_data_21, train_data_20, train_data_19, train_data_18, train_data_17, train_data_16, train_data_15], axis=0)\n",
    "# train_data_combined = pd.concat([train_data_21, train_data_20], axis=0)\n",
    "train_data_combined = pd.read_csv(\"data/ctr_21.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43992294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducir el conjunto de datos mientras se mantiene la distribución de 'label'\n",
    "train_data_combined, _ = train_test_split(train_data_combined, train_size=100000, stratify=train_data_combined['Label'], random_state=42)\n",
    "\n",
    "# Verificar la distribución en el conjunto reducido\n",
    "print(train_data_combined['Label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caracteristicas principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la cantidad de filas del dataset combinado\n",
    "print(f\"Cantidad de filas en el dataset combinado: {train_data_combined.shape[0]}\")\n",
    "\n",
    "# Ver porcentaje de clics vs no clics en la columna Label\n",
    "label_counts = train_data_combined['Label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPorcentaje de clics (1) y no clics (0):\")\n",
    "print(label_counts)\n",
    "\n",
    "# Cantidad de clics (1) y no clics (0)\n",
    "label_counts_abs = train_data_combined['Label'].value_counts()\n",
    "print(\"\\nCantidad de clics (1) y no clics (0):\")\n",
    "print(label_counts_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico de las características numéricas\n",
    "print(train_data_combined.describe())\n",
    "\n",
    "# Resumen de las características categóricas\n",
    "print(train_data_combined.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el porcentaje de valores faltantes por columna\n",
    "missing_data = train_data_combined.isnull().mean() * 100\n",
    "print(missing_data[missing_data > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en numéricos y categóricos\n",
    "numeric_features = train_data_combined.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "categorical_features = train_data_combined.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columna por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos\n",
    "for column in train_data_combined.columns:\n",
    "    unique_values = train_data_combined[column].unique()\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos y la cantidad de veces que aparecen\n",
    "for column in train_data_combined.columns:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(train_data_combined[column].value_counts())\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar un conjunto vacío para almacenar los entity ids únicos\n",
    "unique_entity_ids_2 = set()\n",
    "\n",
    "# Recorrer cada fila de la columna 'action_list_2'\n",
    "for lista in train_data_combined['action_list_2'].dropna():\n",
    "    # Convertir el string de la lista a una lista real y agregar cada valor al conjunto\n",
    "    unique_entity_ids_2.update(eval(lista))  # eval transforma la string en lista si es necesario\n",
    "\n",
    "# Convertir el conjunto en una lista ordenada\n",
    "unique_entity_ids_2 = sorted(unique_entity_ids_2)\n",
    "\n",
    "# Mostrar los entity ids únicos\n",
    "print(unique_entity_ids_2)\n",
    "print(len(unique_entity_ids_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar un conjunto vacío para almacenar los entity ids únicos\n",
    "unique_entity_ids = set()\n",
    "\n",
    "# Recorrer cada fila de la columna 'action_list_1'\n",
    "for lista in train_data_combined['action_list_1'].dropna():\n",
    "    # Convertir el string de la lista a una lista real y agregar cada valor al conjunto\n",
    "    unique_entity_ids.update(eval(lista))  # eval transforma la string en lista si es necesario\n",
    "\n",
    "# Convertir el conjunto en una lista ordenada\n",
    "unique_entity_ids = sorted(unique_entity_ids)\n",
    "\n",
    "# Mostrar los entity ids únicos\n",
    "print(unique_entity_ids)\n",
    "print(len(unique_entity_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la cantidad de valores NaN en cada columna\n",
    "nan_counts = train_data_combined.isna().sum()\n",
    "\n",
    "# Filtrar columnas que tienen al menos un NaN\n",
    "nan_counts_filtered = nan_counts[nan_counts > 0]\n",
    "\n",
    "# Mostrar las columnas con valores NaN de mayor a menor\n",
    "print(nan_counts_filtered.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos\n",
    "for column in train_data_combined.columns:\n",
    "    unique_values = train_data_combined[column].unique()\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories, train_data_combined = data_cleaning(train_data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el dataset modificado como CSV antes de aplicar One-Hot Encoding\n",
    "train_data_combined.to_csv('train_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos\n",
    "for column in train_data_combined.columns:\n",
    "    unique_values = train_data_combined[column].unique()  # Obtener los valores únicos para cada columna\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined.isnull().sum()[train_data_combined.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                              # 20% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Crear el Target Encoder para las columnas categóricas\n",
    "target_encoder = ce.TargetEncoder(cols=categorical_features)\n",
    "\n",
    "# Ajustar y transformar los datos de entrenamiento\n",
    "X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transformar los datos de validación\n",
    "X_val_encoded = target_encoder.transform(X_val)\n",
    "\n",
    "# Crear un imputador que llene los valores NaN con la media de cada columna\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Aplicar el imputador a los datos de entrenamiento\n",
    "X_train_encoded = imputer.fit_transform(X_train_encoded)\n",
    "\n",
    "# También aplica el imputador al conjunto de validación\n",
    "X_val_encoded = imputer.transform(X_val_encoded)\n",
    "\n",
    "# Convertir los resultados a DataFrame y asignar las columnas originales\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=X_train.columns)\n",
    "X_val_encoded = pd.DataFrame(X_val_encoded, columns=X_val.columns)\n",
    "\n",
    "# Ver los primeros datos\n",
    "print(X_train_encoded.head())\n",
    "print(X_val_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de testeo\n",
    "test_data = pd.read_csv(\"data/ctr_test.csv\")\n",
    "\n",
    "# Eliminar las columnas no deseadas\n",
    "test_data = test_data.drop(columns=columns_to_drop)\n",
    "\n",
    "test_data_encoded = target_encoder.transform(test_data.drop(columns=[\"id\"]))\n",
    "test_data_encoded = imputer.transform(test_data_encoded)  # Imputar los valores faltantes en el set de testeo\n",
    "\n",
    "# Convertir los resultados a DataFrame y asignar las columnas originales\n",
    "test_data_encoded = pd.DataFrame(test_data_encoded, columns=test_data.drop(columns=[\"id\"]).columns)\n",
    "\n",
    "# comparar tamaño de columnas\n",
    "print(X_train_encoded.columns)\n",
    "print(test_data_encoded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43992294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined = pd.read_csv('train_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'-2560_action_list_2', '-2606_action_list_1', '-5469_action_list_2', '-5470_action_list_2', '-5471_action_list_2', '-5559_action_list_1', '-5559_action_list_2', '-5560_action_list_2', '-5576_action_list_2', '-5577_action_list_1', '-5577_action_list_2', '-5578_action_list_1', '-5578_action_list_2', '-5579_action_list_1', '-5579_action_list_2', '-5603_action_list_2', '-5604_action_list_2', '-5605_action_list_2', '-5613_action_list_2', '-5736_action_list_1', '-5902_action_list_2', '-6118_action_list_1', '-6118_action_list_2', '-6119_action_list_1', '-6119_action_list_2', '-6125_action_list_2', '-6217_action_list_2', '-6218_action_list_2', '-6219_action_list_2', '-6220_action_list_2', '-6223_action_list_2', '-6224_action_list_2', '-6226_action_list_2', '-6309_action_list_2', '-6451_action_list_1', '-6454_action_list_1', '-6454_action_list_2', '-6543_action_list_2', '-6544_action_list_2', '-6547_action_list_2', '-6548_action_list_2', '-6613_action_list_1', '-6614_action_list_1', '-6615_action_list_1', '-6615_action_list_2', '-6616_action_list_1', '-6617_action_list_1', '-6617_action_list_2', '-6618_action_list_1', '-6618_action_list_2', '-6620_action_list_2', '-6621_action_list_2', '-6770_action_list_2', '-6774_action_list_2', '-6775_action_list_2', '-6779_action_list_1', '-6780_action_list_1', '-6780_action_list_2', '-6800_action_list_2', '-6823_action_list_1', '-6823_action_list_2', '-6824_action_list_1', '-6848_action_list_2', '-6849_action_list_2', '-6850_action_list_2', '-6865_action_list_2', '-6866_action_list_2', '-6867_action_list_2', '-6871_action_list_1', '-6871_action_list_2', '-6874_action_list_1', '-6875_action_list_2', '-6876_action_list_2', '-6902_action_list_1', '-6903_action_list_1', '-6904_action_list_2', '-6905_action_list_2', '-6929_action_list_2', '-6938_action_list_2', '-6946_action_list_2', '-7033_action_list_1', '-7111_action_list_2', '-7112_action_list_2', '-7126_action_list_2', '-7127_action_list_2', '-7143_action_list_2', '-7190_action_list_1', '-7190_action_list_2', '-7194_action_list_1', '-7195_action_list_1', '-7195_action_list_2', '-7196_action_list_2', '-7199_action_list_1', '-7263_action_list_2', '-7264_action_list_2', '-7265_action_list_2', '2560_action_list_2', '2606_action_list_1', '5469_action_list_2', '5470_action_list_2', '5471_action_list_2', '5559_action_list_1', '5559_action_list_2', '5560_action_list_2', '5576_action_list_2', '5577_action_list_1', '5577_action_list_2', '5578_action_list_1', '5578_action_list_2', '5579_action_list_1', '5579_action_list_2', '5603_action_list_2', '5604_action_list_2', '5605_action_list_2', '5613_action_list_2', '5736_action_list_1', '5902_action_list_2', '6118_action_list_1', '6118_action_list_2', '6119_action_list_1', '6119_action_list_2', '6125_action_list_2', '6217_action_list_2', '6218_action_list_2', '6219_action_list_2', '6220_action_list_2', '6223_action_list_2', '6224_action_list_2', '6226_action_list_2', '6309_action_list_2', '6451_action_list_1', '6451_action_list_2', '6454_action_list_1', '6454_action_list_2', '6543_action_list_2', '6544_action_list_2', '6547_action_list_2', '6548_action_list_2', '6614_action_list_1', '6615_action_list_1', '6615_action_list_2', '6616_action_list_1', '6616_action_list_2', '6617_action_list_1', '6618_action_list_1', '6618_action_list_2', '6620_action_list_2', '6621_action_list_2', '6770_action_list_2', '6774_action_list_2', '6775_action_list_2', '6779_action_list_1', '6780_action_list_1', '6780_action_list_2', '6800_action_list_2', '6823_action_list_1', '6823_action_list_2', '6824_action_list_1', '6848_action_list_2', '6849_action_list_2', '6850_action_list_2', '6865_action_list_2', '6866_action_list_2', '6867_action_list_2', '6871_action_list_1', '6871_action_list_2', '6874_action_list_1', '6875_action_list_2', '6876_action_list_2', '6902_action_list_1', '6903_action_list_1', '6904_action_list_2', '6905_action_list_2', '6929_action_list_2', '6938_action_list_2', '6946_action_list_2', '7033_action_list_1', '7111_action_list_2', '7112_action_list_2', '7126_action_list_2', '7127_action_list_2', '7143_action_list_2', '7190_action_list_1', '7190_action_list_2', '7194_action_list_1', '7195_action_list_1', '7195_action_list_2', '7196_action_list_2', '7199_action_list_1', '7263_action_list_2', '7264_action_list_2', '7265_action_list_2', 'IAB-5_auction_list_0', 'IAB1-1_auction_list_0', 'IAB1-2_auction_list_0', 'IAB1-3_auction_list_0', 'IAB1-4_auction_list_0', 'IAB1-5_auction_list_0', 'IAB1-6_auction_list_0', 'IAB1-7_auction_list_0', 'IAB10-1_auction_list_0', 'IAB10-4_auction_list_0', 'IAB10_auction_list_0', 'IAB11-2_auction_list_0', 'IAB11-4_auction_list_0', 'IAB11_auction_list_0', 'IAB12-1_auction_list_0', 'IAB12-2_auction_list_0', 'IAB12-3_auction_list_0', 'IAB12_auction_list_0', 'IAB13-7_auction_list_0', 'IAB13_auction_list_0', 'IAB14-1_auction_list_0', 'IAB14-3_auction_list_0', 'IAB14-7_auction_list_0', 'IAB14_auction_list_0', 'IAB15-10_auction_list_0', 'IAB15-6_auction_list_0', 'IAB15_auction_list_0', 'IAB16-2_auction_list_0', 'IAB16-4_auction_list_0', 'IAB16-6_auction_list_0', 'IAB16_auction_list_0', 'IAB17-12_auction_list_0', 'IAB17-13_auction_list_0', 'IAB17-1_auction_list_0', 'IAB17-27_auction_list_0', 'IAB17-39_auction_list_0', 'IAB17-44_auction_list_0', 'IAB17_auction_list_0', 'IAB18-3_auction_list_0', 'IAB18_auction_list_0', 'IAB19-10_auction_list_0', 'IAB19-11_auction_list_0', 'IAB19-12_auction_list_0', 'IAB19-13_auction_list_0', 'IAB19-14_auction_list_0', 'IAB19-15_auction_list_0', 'IAB19-16_auction_list_0', 'IAB19-17_auction_list_0', 'IAB19-18_auction_list_0', 'IAB19-19_auction_list_0', 'IAB19-1_auction_list_0', 'IAB19-20_auction_list_0', 'IAB19-21_auction_list_0', 'IAB19-22_auction_list_0', 'IAB19-23_auction_list_0', 'IAB19-24_auction_list_0', 'IAB19-25_auction_list_0', 'IAB19-26_auction_list_0', 'IAB19-27_auction_list_0', 'IAB19-28_auction_list_0', 'IAB19-29_auction_list_0', 'IAB19-2_auction_list_0', 'IAB19-30_auction_list_0', 'IAB19-31_auction_list_0', 'IAB19-32_auction_list_0', 'IAB19-33_auction_list_0', 'IAB19-34_auction_list_0', 'IAB19-35_auction_list_0', 'IAB19-36_auction_list_0', 'IAB19-3_auction_list_0', 'IAB19-4_auction_list_0', 'IAB19-5_auction_list_0', 'IAB19-6_auction_list_0', 'IAB19-7_auction_list_0', 'IAB19-8_auction_list_0', 'IAB19_auction_list_0', 'IAB1_auction_list_0', 'IAB2-12_auction_list_0', 'IAB2-14_auction_list_0', 'IAB2-17_auction_list_0', 'IAB2-4_auction_list_0', 'IAB2-8_auction_list_0', 'IAB20-1_auction_list_0', 'IAB20-3_auction_list_0', 'IAB20_auction_list_0', 'IAB21_auction_list_0', 'IAB22_auction_list_0', 'IAB24_auction_list_0', 'IAB2_auction_list_0', 'IAB3-3_auction_list_0', 'IAB3-5_auction_list_0', 'IAB3_auction_list_0', 'IAB4-4_auction_list_0', 'IAB4-5_auction_list_0', 'IAB4_auction_list_0', 'IAB5-1_auction_list_0', 'IAB5-6_auction_list_0', 'IAB5-7_auction_list_0', 'IAB5-8_auction_list_0', 'IAB5-9_auction_list_0', 'IAB5_auction_list_0', 'IAB6-3_auction_list_0', 'IAB6-4_auction_list_0', 'IAB6-5_auction_list_0', 'IAB6-6_auction_list_0', 'IAB6_auction_list_0', 'IAB7-10_auction_list_0', 'IAB7-11_auction_list_0', 'IAB7-12_auction_list_0', 'IAB7-13_auction_list_0', 'IAB7-14_auction_list_0', 'IAB7-15_auction_list_0', 'IAB7-16_auction_list_0', 'IAB7-17_auction_list_0', 'IAB7-18_auction_list_0', 'IAB7-19_auction_list_0', 'IAB7-1_auction_list_0', 'IAB7-20_auction_list_0', 'IAB7-21_auction_list_0', 'IAB7-22_auction_list_0', 'IAB7-23_auction_list_0', 'IAB7-24_auction_list_0', 'IAB7-25_auction_list_0', 'IAB7-26_auction_list_0', 'IAB7-27_auction_list_0', 'IAB7-28_auction_list_0', 'IAB7-29_auction_list_0', 'IAB7-2_auction_list_0', 'IAB7-30_auction_list_0', 'IAB7-31_auction_list_0', 'IAB7-32_auction_list_0', 'IAB7-33_auction_list_0', 'IAB7-34_auction_list_0', 'IAB7-35_auction_list_0', 'IAB7-36_auction_list_0', 'IAB7-37_auction_list_0', 'IAB7-38_auction_list_0', 'IAB7-39_auction_list_0', 'IAB7-3_auction_list_0', 'IAB7-40_auction_list_0', 'IAB7-41_auction_list_0', 'IAB7-42_auction_list_0', 'IAB7-43_auction_list_0', 'IAB7-44_auction_list_0', 'IAB7-45_auction_list_0', 'IAB7-4_auction_list_0', 'IAB7-5_auction_list_0', 'IAB7-6_auction_list_0', 'IAB7-7_auction_list_0', 'IAB7-8_auction_list_0', 'IAB7-9_auction_list_0', 'IAB7_auction_list_0', 'IAB8-6_auction_list_0', 'IAB8_auction_list_0', 'IAB9-10_auction_list_0', 'IAB9-11_auction_list_0', 'IAB9-12_auction_list_0', 'IAB9-13_auction_list_0', 'IAB9-14_auction_list_0', 'IAB9-15_auction_list_0', 'IAB9-16_auction_list_0', 'IAB9-17_auction_list_0', 'IAB9-18_auction_list_0', 'IAB9-19_auction_list_0', 'IAB9-1_auction_list_0', 'IAB9-20_auction_list_0', 'IAB9-21_auction_list_0', 'IAB9-22_auction_list_0', 'IAB9-23_auction_list_0', 'IAB9-24_auction_list_0', 'IAB9-25_auction_list_0', 'IAB9-26_auction_list_0', 'IAB9-27_auction_list_0', 'IAB9-28_auction_list_0', 'IAB9-29_auction_list_0', 'IAB9-2_auction_list_0', 'IAB9-30_auction_list_0', 'IAB9-31_auction_list_0', 'IAB9-3_auction_list_0', 'IAB9-4_auction_list_0', 'IAB9-5_auction_list_0', 'IAB9-6_auction_list_0', 'IAB9-7_auction_list_0', 'IAB9-8_auction_list_0', 'IAB9-9_auction_list_0', 'IAB9_auction_list_0', 'books_auction_list_0', 'business_auction_list_0', 'education_auction_list_0', 'entertainment_auction_list_0', 'finance_auction_list_0', 'games_auction_list_0', 'healthcare_and_fitness_auction_list_0', 'lifestyle_auction_list_0', 'mañana', 'medical_auction_list_0', 'music_auction_list_0', 'navigation_auction_list_0', 'news_auction_list_0', 'noche', 'photography_auction_list_0', 'productivity_auction_list_0', 'reference_auction_list_0', 'social_networking_auction_list_0', 'sports_auction_list_0', 'tarde', 'travel_auction_list_0', 'utilities_auction_list_0', 'weather_auction_list_0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                              # 20% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Asegurarse de que X_train y X_val sean dataframes de pandas\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_val = pd.DataFrame(X_val)\n",
    "\n",
    "# Asegurarse de que y_train y y_val sean series de pandas\n",
    "y_train = pd.Series(y_train)\n",
    "y_val = pd.Series(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna: action_categorical_6, Tipos: [<class 'str'> <class 'int'>]\n",
      "Columna: auction_boolean_0, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_boolean_1, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_boolean_2, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_categorical_0, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_categorical_11, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_categorical_12, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_categorical_2, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_categorical_3, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_categorical_4, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_categorical_6, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_categorical_7, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: auction_categorical_9, Tipos: [<class 'float'> <class 'str'>]\n",
      "Columna: creative_categorical_12, Tipos: [<class 'float'> <class 'str'>]\n",
      "Columna: creative_categorical_2, Tipos: [<class 'float'> <class 'str'>]\n",
      "Columna: creative_categorical_3, Tipos: [<class 'float'> <class 'str'>]\n",
      "Columna: creative_categorical_4, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: creative_categorical_5, Tipos: [<class 'float'> <class 'str'>]\n",
      "Columna: creative_categorical_6, Tipos: [<class 'float'> <class 'str'>]\n",
      "Columna: creative_categorical_7, Tipos: [<class 'float'> <class 'str'>]\n",
      "Columna: creative_categorical_9, Tipos: [<class 'str'> <class 'float'>]\n",
      "Columna: gender, Tipos: [<class 'str'> <class 'float'>]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "for column in X_train.columns:\n",
    "    # Obtén los tipos de datos únicos por columna\n",
    "    unique_types = X_train[column].apply(type).unique()\n",
    "    if len(unique_types) > 1:\n",
    "        print(f\"Columna: {column}, Tipos: {unique_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = [col for col in X_train.select_dtypes(include=['object', 'int', 'float']).columns if col not in categories]\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Crear el preprocesador para manejar NaNs y aplicar One-Hot Encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('convert_to_string', FunctionTransformer(lambda x: x.astype(str), validate=False)),  # Convertir todos los valores a cadena\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='passthrough'  # Mantener el resto de las columnas como están\n",
    ")\n",
    "\n",
    "# Aplicar el pipeline de preprocesamiento a los datos de entrenamiento\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "# Aplicar el pipeline al conjunto de validación\n",
    "X_val_encoded = preprocessor.transform(X_val)\n",
    "\n",
    "# Convertir los resultados a DataFrame si es necesario\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=preprocessor.get_feature_names_out())\n",
    "X_val_encoded = pd.DataFrame(X_val_encoded, columns=preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X_train_encoded.columns:\n",
    "    unique_values = X_train_encoded[column].unique()  # Obtener los valores únicos\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Cargar los datos de testeo\n",
    "test_data = pd.read_csv(\"data/ctr_test.csv\")\n",
    "\n",
    "# Paso 2: Modificar la columna auction_list_0 (expansión de la lista en múltiples columnas)\n",
    "categories2, test_data = data_cleaning(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Ajustar las columnas del conjunto de testeo\n",
    "test_data = ajustar_columnas_test(X_train, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Aplicar el preprocesador al conjunto de testeo\n",
    "test_data_encoded = preprocessor.transform(test_data)\n",
    "\n",
    "# Convertir los resultados a DataFrame si es necesario\n",
    "test_data_encoded = pd.DataFrame(test_data_encoded, columns=preprocessor.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_encoded.to_csv('test_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_encoded = pd.read_csv('test_data_cleaned.csv')\n",
    "\n",
    "test_data = pd.read_csv(\"data/ctr_test.csv\")\n",
    "\n",
    "print(f\"Columnas en el conjunto de entrenamiento: {X_train_encoded.shape[1]}\")\n",
    "print(f\"Columnas en el conjunto de testeo: {test_data_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busqueda de hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'max_samples': np.linspace(0.5, 1.0, num=10),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Definir el modelo\n",
    "model = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "# Definir el RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=2,  # Número de combinaciones de hiperparámetros a probar\n",
    "    scoring='roc_auc',\n",
    "    cv=3,  # Número de folds en la validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "random_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validación con el mejor modelo encontrado\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "# Calcular el AUC en el conjunto de validación\n",
    "auc = roc_auc_score(y_val, y_pred_proba)\n",
    "print(f\"AUC en el conjunto de validación: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [None, 10, 20, 30, 40, 50]),\n",
    "    'max_samples': hp.uniform('max_samples', 0.5, 1.0),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),\n",
    "    'bootstrap': True\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt\n",
    "def objective(params):\n",
    "    # Asegurarse de que 'max_samples' solo se usa si 'bootstrap' es True\n",
    "    if not params['bootstrap']:\n",
    "        params['max_samples'] = None  # No se puede utilizar max_samples si bootstrap es False\n",
    "    \n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model = RandomForestClassifier(**params, random_state=random_state)\n",
    "    \n",
    "    # Entrenar el modelo directamente ya que el Target Encoder e Imputador ya se aplicaron\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    # Hacer predicciones en el conjunto de validación\n",
    "    y_pred_proba = model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "    # Calcular el AUC en el conjunto de validación\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "\n",
    "    # Retornar el valor negativo del AUC ya que Hyperopt minimiza por defecto\n",
    "    return {'loss': 1-auc, 'status': STATUS_OK}\n",
    "\n",
    "# Ejecutar la optimización\n",
    "trials = Trials()  # Para guardar información sobre cada iteración\n",
    "best = fmin(fn=objective, \n",
    "            space=space, \n",
    "            algo=tpe.suggest, \n",
    "            max_evals=10,  # Número de evaluaciones\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros para XGBoost\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 5, 10, 15, 20],  # Profundidad máxima del árbol\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),  # Tasa de aprendizaje\n",
    "    'subsample': np.linspace(0.5, 1.0, num=5),  # Fracción de muestras a usar para cada árbol\n",
    "    'colsample_bytree': np.linspace(0.5, 1.0, num=5),  # Fracción de características a usar para cada árbol\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],  # Regularización\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],  # Regularización L1\n",
    "    'reg_lambda': [1, 1.5, 2, 3]  # Regularización L2\n",
    "}\n",
    "\n",
    "# Definir el modelo\n",
    "model = XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Definir el RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=2,  # Número de combinaciones de hiperparámetros a probar\n",
    "    scoring='roc_auc',  # Métrica para evaluar\n",
    "    cv=3,  # Número de folds en la validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "random_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validación con el mejor modelo encontrado\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "# Calcular el AUC en el conjunto de validación\n",
    "auc = roc_auc_score(y_val, y_pred_proba)\n",
    "print(f\"AUC en el conjunto de validación: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espacio de búsqueda para los hiperparámetros\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 1, 10),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2)\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt\n",
    "def objective(params):\n",
    "    # Entrenar el modelo con los hiperparámetros actuales\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=int(params['max_depth']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Entrenar el modelo con los datos de entrenamiento\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "    \n",
    "    # Predecir las probabilidades para calcular el AUC en el conjunto de validación\n",
    "    y_pred_proba = model.predict_proba(X_val_encoded)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=2,  # Número de evaluaciones\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reentrenar el modelo con los mejores hiperparámetros\n",
    "best_model = xgb.XGBClassifier(\n",
    "    max_depth=int(best['max_depth']),\n",
    "    learning_rate=best['learning_rate'],\n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    gamma=best['gamma'],\n",
    "    min_child_weight=best['min_child_weight'],\n",
    "    subsample=best['subsample'],\n",
    "    colsample_bytree=best['colsample_bytree'],\n",
    "    scale_pos_weight=best['scale_pos_weight'],\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros en los datos completos\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de validación\n",
    "y_pred_proba_val = best_model.predict_proba(X_val)[:, 1]\n",
    "auc_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "print(f\"AUC en el conjunto de validación: {auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pueba de Hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_samples': 0.6666666666666666, 'max_depth': 50, 'bootstrap': True}\n",
    "# Reconstruir el modelo con los mejores hiperparámetros\n",
    "best_model_rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=20,\n",
    "    max_samples=None,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=4,\n",
    "    bootstrap=False,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "rf_pipeline = make_pipeline(SimpleImputer(), best_model_rf)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "rf_pipeline.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_rf_test = rf_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_rf = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_rf_test})\n",
    "submission_df_rf[\"id\"] = submission_df_rf[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"random_forest\"\n",
    "file_name_rf = (\n",
    "    f\"random_forest_predictions_n_estimators_{500}_\"\n",
    "    f\"max_depth_{20}_\"\n",
    "    f\"min_samples_split_{5}_\"\n",
    "    f\"min_samples_leaf_{4}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_rf.to_csv(os.path.join(\"submits\", file_name_rf), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los hiperparámetros a los tipos correctos\n",
    "random_search.best_params_['n_estimators'] = int(random_search.best_params_['n_estimators'])\n",
    "random_search.best_params_['min_samples_split'] = int(random_search.best_params_['min_samples_split'])\n",
    "random_search.best_params_['min_samples_leaf'] = int(random_search.best_params_['min_samples_leaf'])\n",
    "random_search.best_params_['max_depth'] = int(random_search.best_params_['max_depth']) if random_search.best_params_['max_depth'] is not None else None\n",
    "\n",
    "# Reconstruir el modelo con los mejores hiperparámetros\n",
    "best_model_rf = RandomForestClassifier(\n",
    "    n_estimators=random_search.best_params_['n_estimators'],\n",
    "    max_depth=random_search.best_params_['max_depth'],  # Asegurarse de que max_depth sea un entero o None\n",
    "    min_samples_split=random_search.best_params_['min_samples_split'],\n",
    "    min_samples_leaf=random_search.best_params_['min_samples_leaf'],\n",
    "    bootstrap=random_search.best_params_['bootstrap'],\n",
    "    random_state=random_state,\n",
    "    max_samples=random_search.best_params_['max_samples']\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "rf_pipeline = make_pipeline(SimpleImputer(), best_model_rf)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "rf_pipeline.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_rf_test = rf_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_rf = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_rf_test})\n",
    "submission_df_rf[\"id\"] = submission_df_rf[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"random_forest\"\n",
    "file_name_rf = (\n",
    "    f\"random_forest_predictions_n_estimators_{random_search.best_params_['n_estimators']}_\"\n",
    "    f\"max_depth_{random_search.best_params_['max_depth']}_\"\n",
    "    f\"min_samples_split_{random_search.best_params_['min_samples_split']}_\"\n",
    "    f\"max_samples_{random_search.best_params_['max_samples']}_\"\n",
    "    f\"bootstrap_{random_search.best_params_['bootstrap']}_\"\n",
    "    f\"min_samples_leaf_{random_search.best_params_['min_samples_leaf']}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_rf.to_csv(os.path.join(\"submits\", file_name_rf), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.016434700376417533,\n",
    "    subsample=0.6696027253148403,\n",
    "    scale_pos_weight=1.9885767576831004,\n",
    "    colsample_bytree=0.6189520886008961,\n",
    "    min_child_weight=6.378545393104531,\n",
    "    gamma=0.05263130692482787,\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Si estás usando una versión reciente de XGBoost\n",
    "    eval_metric=\"auc\"  # Ajustar el métrico de evaluación\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "xgb_pipeline = make_pipeline(SimpleImputer(), best_model_xgb)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"xgboost\"\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_predictions_n_estimators_{500}_\"\n",
    "    f\"max_depth_{20}_\"\n",
    "    f\"learning_rate_{0.05}_\"\n",
    "    f\"min_child_weight_{4}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atomatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparámetros: {'colsample_bytree': 0.6813166881988515, 'gamma': 0.3836347732082877, 'learning_rate': 0.02522774305471077, 'max_depth': 4, 'min_child_weight': 9.181974700523355, 'n_estimators': 400, 'scale_pos_weight': 1.4846499378171216, 'subsample': 0.9484768092226656}, AUC: 0.8442\n",
    "\n",
    "# Convertir los hiperparámetros a enteros si es necesario\n",
    "best['n_estimators'] = int(best['n_estimators'])\n",
    "best['max_depth'] = [None, 10, 20, 30, 40, 50][best['max_depth']]  # Ajustar el valor del índice si es necesario\n",
    "best['scale_pos_weight'] = best['scale_pos_weight']\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=best['n_estimators'],\n",
    "    max_depth=best['max_depth'],\n",
    "    learning_rate=best['learning_rate'],\n",
    "    subsample=best['subsample'],\n",
    "    colsample_bytree=best['colsample_bytree'],\n",
    "    min_child_weight=best['min_child_weight'],\n",
    "    gamma=best['gamma'],\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=best['scale_pos_weight'],\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear el pipeline con el mejor modelo\n",
    "xgb_pipeline = make_pipeline(SimpleImputer(), best_model_xgb)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(test_data_encoded)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"xgboost\"\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_predictions_n_estimators_{best['n_estimators']}_\"\n",
    "    f\"max_depth_{best['max_depth']}_\"\n",
    "    f\"learning_rate_{best['learning_rate']}_\"\n",
    "    f\"subsample_{best['subsample']}_\"\n",
    "    f\"colsample_bytree_{best['colsample_bytree']}_\"\n",
    "    f\"gamma_{best['gamma']}_\"\n",
    "    f\"scale_pos_weight_{best['scale_pos_weight']}_\"\n",
    "    f\"min_child_weight_{best['min_child_weight']}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
